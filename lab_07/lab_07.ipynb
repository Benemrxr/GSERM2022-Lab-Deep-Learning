{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"hsg_logo.png\">\n",
    "\n",
    "##  Lab 7: Sentiment analysis using long short-term memory networks\n",
    "\n",
    "GSERM Summer School 2022, Deep Learning: Fundamentals and Applications, University of St. Gallen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab environment of the \"Deep Learning: Fundamentals and Applications\" GSERM course at the University of St. Gallen (HSG) is based on Jupyter Notebooks (https://jupyter.org), which allow to perform a variety of statistical evaluations and data analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [kaggle Rotten Tomates](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews) dataset for this exercise; you may need to register to download the [data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) (no worries, it's free).\n",
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset.\n",
    "The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order.\n",
    "Each Sentence has been parsed into many phrases (chunks) using the Stanford parser ([learn more](https://nlp.stanford.edu/software/lex-parser.shtml)).\n",
    "Each phrase has a `PhraseId`, each sentence a `SentenceId`; phrases that are repeated (such as short or common words) are only included once in the data.\n",
    "\n",
    "- `train.tsv` contains the phrases and their associated sentiment labels; we will further split this into training and validation partitions to train our model and optimize the hyperparameters\n",
    "- `test.tsv` contains just phrases; use your model to assign a sentiment label to each phrase (homework)\n",
    "\n",
    "Feel free to browse through the data to familiarize yourself with the task.\n",
    "The sentiment labels are:\n",
    "\n",
    "* 0 - negative\n",
    "* 1 - somewhat negative\n",
    "* 2 - neutral\n",
    "* 3 - somewhat positive\n",
    "* 4 - positive\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "As in the other labs, we will be using pytorch and a few related modules.\n",
    "\n",
    "- <https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>\n",
    "- <https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>\n",
    "- `torchtext`'s [GloVe embeddings](https://torchtext.readthedocs.io/en/latest/vocab.html#glove) as initalization to our embedding layer\n",
    "- We'll be computing the evaluation metrics using the [scikit learn metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) package.\n",
    "\n",
    "Work your way through the notebook.\n",
    "We've deliberately chosen very moderate hyperparameters to speed up the computation.\n",
    "Go through each of the sections and familiarize yourself with the data preparation, model and traing setup as well as evaluation.\n",
    "Homework assignments are listed at the end of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_data(path='res/train.tsv'):\n",
    "    df = pd.read_csv(path, sep='\\t', header=0)\n",
    "    \n",
    "    # columns = ['PhraseId' 'SentenceId', 'Phrase', 'Sentiment']\n",
    "    def process_phrase(phrase):\n",
    "        remove_punct = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        remove_digits = str.maketrans(string.digits, ' '*len(string.digits))\n",
    "        phrase = phrase.translate(remove_digits)\n",
    "        phrase = phrase.translate(remove_punct)\n",
    "        phrase = re.sub(' {2,}', ' ', phrase)\n",
    "        return phrase.lower()\n",
    "    \n",
    "    # apply to all phrases\n",
    "    df['Phrase'] = df['Phrase'].apply(lambda x: process_phrase(x))\n",
    "    \n",
    "    # filter out empty phrases\n",
    "    df = df[df['Phrase'].str.len() > 1]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     7072\n",
      "1    27271\n",
      "2    79410\n",
      "3    32921\n",
      "4     9206\n",
      "Name: Sentiment, dtype: int64\n",
      "0     5680\n",
      "1    21759\n",
      "2    63597\n",
      "3    26317\n",
      "4     7351\n",
      "Name: Sentiment, dtype: int64\n",
      "0     1392\n",
      "1     5512\n",
      "2    15813\n",
      "3     6604\n",
      "4     1855\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = load_sentiment_data()\n",
    "\n",
    "# split the train.csv in train and test\n",
    "df_train = df.sample(frac=0.8, random_state=42)\n",
    "df_vali = df.drop(df_train.index)\n",
    "\n",
    "# list some stats on the label distribution\n",
    "print(df.Sentiment.value_counts().sort_index())\n",
    "print(df_train.Sentiment.value_counts().sort_index())\n",
    "print(df_vali.Sentiment.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first time you run this will download a ~823MB file\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "# We'll use the stoi (string to id) and itos (id to string) methods to convert\n",
    "# between word token and id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model the RT dataset\n",
    "class RottenTomatoesDataset(Dataset):\n",
    "    def __init__(self, df, glove_vocab, label_col='Sentiment', unk='<unk>'):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.labels = self.df[label_col].values\n",
    "        self.glove = glove_vocab\n",
    "        self.vocab_size = len(glove_vocab)\n",
    "        self.data = []\n",
    "\n",
    "        # map the tokens\n",
    "        for p in self.df['Phrase'].values:\n",
    "            self.data.append(torch.stack(\n",
    "                [torch.LongTensor([glove.stoi.get(w, glove.stoi.get(unk))]) for w in p.split()]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# when processing sequences of different lengths in batches, we need to pad the\n",
    "# shorter ones to match the length of the longest in the batch\n",
    "class SequencePadder():\n",
    "    def __init__(self, symbol) -> None:\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "        sequences = [x[0] for x in sorted_batch]\n",
    "        labels = [x[1] for x in sorted_batch]\n",
    "        padded = pad_sequence(sequences, padding_value=self.symbol)\n",
    "        lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        return padded, torch.LongTensor(labels), lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, data_loader, device):\n",
    "    # we do this with batch size 1, so that padding doesn't affect the prediction\n",
    "    with torch.set_grad_enabled(False):\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        y_pred, y_true = [], []\n",
    "        sentences = []\n",
    "        \n",
    "        for inputs, labels, lengths in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            out, weights = model(inputs, lengths)\n",
    "\n",
    "            _, preds = torch.max(out, 1)\n",
    "\n",
    "            y_pred.append(preds.item())\n",
    "            y_true.append(labels.item())\n",
    "\n",
    "        return {\n",
    "            'f1': f1_score(y_true, y_pred, average='micro'),\n",
    "            'prec': precision_score(y_true, y_pred, average='micro'),\n",
    "            'recall': recall_score(y_true, y_pred, average='micro'),\n",
    "            'acc': accuracy_score(y_true, y_pred),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifierGloveEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                hidden_size,\n",
    "                output_size, # number of classes\n",
    "                glove=None,\n",
    "                num_layers=1,\n",
    "                bidirectional=False):\n",
    "\n",
    "        super(LstmClassifierGloveEmbeddings, self).__init__()\n",
    "\n",
    "        self.input_size = len(glove) # vocabulary size\n",
    "        self.embedding_size = glove.dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # we use nn.Embedding with the pre-trained GloVe vectors (and also don't update them in training)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=True)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "                        input_size=self.embedding_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers=self.num_layers,\n",
    "                        dropout=0.2 if num_layers > 1 else 0,\n",
    "                        bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        fc_size = self.hidden_size * self.num_directions\n",
    "        self.fc = nn.Linear(fc_size, output_size)\n",
    "\n",
    "    # h_n is the previous hidden state tuple as (h_n, c_n)\n",
    "    def forward(self, x, lengths, h_n=None):\n",
    "        if h_n is None:\n",
    "            h_n, c_n = self.init_hidden(x.size(1))\n",
    "        else:\n",
    "            h_n =  h_n[0]\n",
    "            c_n =  h_n[1]\n",
    "        \n",
    "        # 1: apply the embedding layer\n",
    "        embed = self.embedding(x).squeeze(2)\n",
    "        \n",
    "        # packed squence helps avoid unneccsary computation, with the length it marks out irrelvant/ padded sequence\n",
    "        # elements, this allows the efficient computation of sequences of different lengths inside the same batch\n",
    "        packed_seq = pack_padded_sequence(embed, lengths)\n",
    "        \n",
    "        # 2: ...and feed the output to the LSTM\n",
    "        out, (h_n, c_n) = self.lstm(packed_seq, (h_n, c_n))\n",
    "\n",
    "        # out contains the output features (h_t) from the last layer of the LSTM, for each timestep\n",
    "        # h_n contains the final hidden state for each element in the batch.\n",
    "        # c_n contains the final cell state for each element in the batch.\n",
    "\n",
    "        # 3 reccurrent layer, hidden dimension = 100, bidirectional, batch_size 2\n",
    "         \n",
    "        # h_n.shape = (6, 2, 100)\n",
    "        # h_n.shape = (num_layers * directions, batch_size, hidden_dimension)\n",
    "        # \n",
    "        # according to the docs, use the following view to address the per-layer hiddens\n",
    "        # (num_layers, num_directions, batch_size, hidden_dim)\n",
    "        # h_n.view(self.num_layers, self.num_directions, x.size(1), -1).shape = [3, 2, 2, 100]\n",
    "        # ... and we'll want the top-most of those, and concatted, if bi-directional\n",
    "\n",
    "        # get the top/last hidden of the LSTM stack\n",
    "        h_n_top = h_n.view(self.num_layers, self.num_directions, x.size(1), -1)[-1]\n",
    "        if self.num_directions == 2:\n",
    "            # for bi-directional models, we combine/concat both hidden states\n",
    "            h_n_top = torch.cat([h_n_top[0], h_n_top[1]], 1)\n",
    "        \n",
    "        # since shape is now [[..]], drop the outer dim\n",
    "        h_n_top = h_n_top.squeeze(0)\n",
    "        logits = self.fc(h_n_top)\n",
    "\n",
    "        return logits, (h_n, c_n) # only hidden state for the last layer is needed for loss calculation\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        # h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        h_dim_0 = self.num_layers * self.num_directions\n",
    "        hidden = (torch.zeros(h_dim_0, batch_size, self.hidden_size, device=device),\n",
    "                  torch.zeros(h_dim_0, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_model(model, dl_train, dl_vali, criterion, optimizer, device, num_epochs=25):\n",
    "    # this should already be done\n",
    "    # model.to(device)\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # phase 1: training\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        # go through all the data\n",
    "        for inputs, labels, lens in dl_train:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            with torch.set_grad_enabled(True):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out, _ = model(inputs, lens) \n",
    "                \n",
    "                # take only the last output, compute loss\n",
    "                _, preds = torch.max(out, 1)\n",
    "                loss = criterion(out, labels)\n",
    "\n",
    "                # do backprop\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            train_loss += loss.item() * inputs.size(1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # print stats for the training pass\n",
    "        print('training: loss={:.4f} acc={:.4f}'.format(train_loss / len(dl_train.dataset), train_correct.double() / len(dl_train.dataset)))\n",
    "\n",
    "        # phase 2: validate...\n",
    "        model.eval()\n",
    "\n",
    "        vali_loss = 0.0\n",
    "        vali_correct = 0\n",
    "\n",
    "        for inputs, labels, lens in dl_vali:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward; no gradient needed now\n",
    "            with torch.set_grad_enabled(False):\n",
    "                out, _ = model(inputs, lens) \n",
    "                \n",
    "                # take only the last output\n",
    "                _, preds = torch.max(out, 1)\n",
    "                loss = criterion(out, labels)\n",
    "\n",
    "            # statistics\n",
    "            vali_loss += loss.item() * inputs.size(1)\n",
    "            vali_correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "        # print stats for vali pass\n",
    "        print('validation: loss={:.4f} acc={:.4f}'.format(vali_loss / len(dl_train.dataset), vali_correct.double() / len(dl_vali.dataset)))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to add the padding and unknown symbols to our glove embedding\n",
    "def append_special(glove, special, vec=None):\n",
    "    glove.itos.append(special)\n",
    "    glove.stoi[special] = glove.itos.index(special)\n",
    "    if vec is None:\n",
    "        vec = torch.zeros(1, glove.vectors.size(1))\n",
    "    glove.vectors = torch.cat((glove.vectors, vec))\n",
    "    return glove\n",
    "\n",
    "pad_sym = '<pad>'\n",
    "unk = '<unk>'\n",
    "\n",
    "# GloVe has been loaded all the way at the top already; now we append to it\n",
    "glove = append_special(glove, unk)\n",
    "glove = append_special(glove, pad_sym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LSTM parameters\n",
    "hidden_size = 32\n",
    "n_layers = 1\n",
    "bi_direct = False\n",
    "\n",
    "# training parameters\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "shuffle = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifierGloveEmbeddings(\n",
      "  (embedding): Embedding(400002, 50)\n",
      "  (lstm): LSTM(50, 32)\n",
      "  (fc): Linear(in_features=32, out_features=5, bias=True)\n",
      ")\n",
      "Epoch 1/10\n",
      "----------\n",
      "training: loss=1.1833 acc=0.5374\n",
      "validation: loss=0.2695 acc=0.5605\n",
      "Epoch 2/10\n",
      "----------\n",
      "training: loss=1.0483 acc=0.5701\n",
      "validation: loss=0.2591 acc=0.5708\n",
      "Epoch 3/10\n",
      "----------\n",
      "training: loss=1.0181 acc=0.5791\n",
      "validation: loss=0.2536 acc=0.5784\n",
      "Epoch 4/10\n",
      "----------\n",
      "training: loss=0.9994 acc=0.5849\n",
      "validation: loss=0.2497 acc=0.5867\n",
      "Epoch 5/10\n",
      "----------\n",
      "training: loss=0.9866 acc=0.5894\n",
      "validation: loss=0.2473 acc=0.5891\n",
      "Epoch 6/10\n",
      "----------\n",
      "training: loss=0.9767 acc=0.5935\n",
      "validation: loss=0.2455 acc=0.5934\n",
      "Epoch 7/10\n",
      "----------\n",
      "training: loss=0.9685 acc=0.5966\n",
      "validation: loss=0.2443 acc=0.5937\n",
      "Epoch 8/10\n",
      "----------\n",
      "training: loss=0.9617 acc=0.5998\n",
      "validation: loss=0.2425 acc=0.5982\n",
      "Epoch 9/10\n",
      "----------\n",
      "training: loss=0.9559 acc=0.6026\n",
      "validation: loss=0.2414 acc=0.6002\n",
      "Epoch 10/10\n",
      "----------\n",
      "training: loss=0.9505 acc=0.6036\n",
      "validation: loss=0.2402 acc=0.6021\n",
      "Training complete in 4m 6s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set up the model and training mechanics\n",
    "model = LstmClassifierGloveEmbeddings(hidden_size,\n",
    "                                      output_size=len(df['Sentiment'].value_counts()),\n",
    "                                      glove=glove,\n",
    "                                      num_layers=n_layers,\n",
    "                                      bidirectional=bi_direct,)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "ds_train = RottenTomatoesDataset(df_train, glove)\n",
    "ds_vali = RottenTomatoesDataset(df_vali, glove)\n",
    "\n",
    "dl_train = DataLoader(ds_train,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      collate_fn=SequencePadder(glove.stoi[pad_sym]),\n",
    "                      drop_last=True)\n",
    "\n",
    "dl_vali = DataLoader(ds_vali,\n",
    "                     batch_size=batch_size, \n",
    "                     shuffle=False,\n",
    "                     collate_fn=SequencePadder(glove.stoi[pad_sym]),\n",
    "                     drop_last=True)\n",
    "\n",
    "# train model returns the best model for the current run\n",
    "model = train_rnn_model(model, \n",
    "                        dl_train, dl_vali, \n",
    "                        criterion, \n",
    "                        optimizer,\n",
    "                        device, \n",
    "                        num_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.6022581472927894, 'prec': 0.6022581472927894, 'recall': 0.6022581472927894, 'acc': 0.6022581472927894}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dl_vali2 = DataLoader(ds_vali,\n",
    "                     batch_size=1, \n",
    "                     shuffle=False,\n",
    "                     collate_fn=SequencePadder(glove.stoi[pad_sym]),\n",
    "                     drop_last=True)\n",
    "\n",
    "# run eval for best model and save for this split\n",
    "scores = get_metrics(model,\n",
    "                      dl_vali2,\n",
    "                      device)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. Train with different parameters (number of hidden units and layers, bi-directional, batch sizes, learning rates) to improve your classification metrics\n",
    "2. For your best result, assemble a kaggle submission (test.csv)\n",
    "3. Optional assignment: extend the LSTM model by using `nn.MultiheadAttention` on the outputs and measure the improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
